Some short-term goals
---------------------

1. Introduce an input event abstraction

Currently we just dump SDL events directly into a stream; we should create
an enumeration that gathers together various things we care about putting
into the network, so they can be present in the same stream. E.g.,
    * Keyboard input
    * Mouse movement / clicks
    * Time deltas?

XXX: not sure about including time deltas there; we'd prefer to not use
deltas for anything except generating the timestream.

As follow-on work, this will allow pulling the "world" out as an encapsulated
reactive behavior. This will dramatically clean up the main game loop, though
it will also require deciding on some suitable type for the rendering signal.
The main loop owns the renderer and render context, and is responsible for
sampling the render signal and presenting its value to the renderer, which it
currently does by generating three Scenes, world, HUD, and GUI, which differ
by their scaling and anchoring.

Scene does not currently contain anything except a z-sorted set of render ops,
so to fully describe this compositing would require a signal type of
Signal<Vec<(Scene, Ratio, Point)>>. Eventually, however, anchoring and scale
may be incorporated into Scene in several possible ways:

    a.  Add a single scale and anchor attribute per Scene.
    b.  Allow a scene to contain multiple layers, each of which represents
        a z-index barrier and has its own scale and anchor.
    c.  Allow individual ops within a scene to have independent scale and
        anchor.

Option A seems simplest, and effectively makes a Scene into a single layer;
the world's render signal would then have type Signal<Vec<Scene>>.

Options B and C would make it possible to express any compositing within a
single Scene, simplifying the behavior type to Signal<Scene>.

All have different architectural implications, especially when it comes to
thinking about how to optimize the render pipeline, and whether layer-by-layer
memoization maps well to real-world usage. More on this to follow; but as a
first draft, choosing (A) is likely to be a good step.


2. Move editor GUI to a separate module

Create an interface that just exposes the relevant streams and signals;
should allow a fairly elegant decoupling of GUI from the main loop. Probably
depends on #1, or at least having mouse events in carboxyl.

Note that there are some dependency issues that probably should be fixed.
Notably, world rendering and the GUI are ostensibly independent systems, but
have an implicit dependency on each other in that they both depend on the
scale signal and must use it in the same way in order for their integration to
be correct. This suggests that the dependency should be made explicit,
probably by moving ownership of the scale behavior to one or the other.

3. Move audio to a separate module, add basic mixing interface

3a. FRP audio mixer

Figure out a nice way to use FRP in the mixer. It would be super cool to
represent every channel as a reactive behavior, but this raises at least two
questions:

    1.  In a push-pull system, what is the "push" that induces the behavior
        to change?

    2.  When sampling from a channel, how does the underlying reactive
        behavior know how many samples to provide?

Both points are related to the necessarily discrete and chunked nature of
audio sampling. Currently an SDL audio callback is invoked at regular
intervals and expected to fill a buffer of a size known at the point where the
audio system is initialized.

Possibly, then, we could "push" a value into the audio network that indicates
how many samples will be required; each behavior (channel) could then produce
an appropraite number of samples from their sources.

NOTE: behaviors would also seem a natural way to implement transformations in
the audio mixer as well; e.g., an upsampling fold or low-pass filter, of
particular relevance to SPC emulator generated samples, as those are currently
fixed at 32KHz.

So, let's say we create a sink for "fill" events. An event would be pushed
into this sink by the audio callback.

What then depends on the stream, and how do the results get back to the audio
callback? How many channels are there in the mixer, and why? A limited set of
channels is a particularly poor abstraction. I wish to dispense with it. What
all can produce sound? How is a thing that can produce sound introduced to the
audio mixer?

Oh yeah, and one more neat idea: why not tie sound sampling to the rest of the
world's update cycle? 1/60s ~= 17ms is a plenty big enough audio buffer
length, and it seems to tie things together nicely.

There are some possible problems with this approach.

    1.  Timing. Starting the SDL audio system with a sample count
        corresponding to 1/60 of a second and relying on the audio callback
        is likely to misalign with the desired framerate.
    2.  Cumulative error. The SDL audio system provides an alternative, which
        is to push arbitrary quantities of samples to an internal buffer
        outside the control of the audio callback. This would allow us to push
        data at the beginning of every frame, but since the hardware is
        consuming it at its leisure, we are still likely to run into timing
        problems. If we push too few samples, the audio will have brief blank
        spaces, creating unpleasant ~60Hz noise. If we push too many, we fall
        increasingly out of sync.

It may be worth investigating lower-level access to audio hardware to address
some of these issues.

4. Greenfield a new FRP physics approach. Probably put momentum first.

Our first-order abstraction for physics entities will be a pair of signals,
momentum and position. (We can assume that in this system, σₓσₛ≠ħ/2) In the
normal course of things, position should be straightforwardly derived from
momentum. We'll implement just two momentum functions at first: constant,
and keyboard-controlled.

This should (hopefully) make it a bit easier to implement collision detection
in terms of momentum and position signals.

5. Add metrics.

Measure, measure, measure! Probably want to track render and FRP network
update times to start with. Might also be nice to add automatic regression
detection somehow.
