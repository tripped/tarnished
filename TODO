Some short-term goals
---------------------

1. Introduce an input event abstraction

Currently we just dump SDL events directly into a stream; we should create
an enumeration that gathers together various things we care about putting
into the network, so they can be present in the same stream. E.g.,
    * Keyboard input
    * Mouse movement / clicks
    * Time deltas?

XXX: not sure about including time deltas there; we'd prefer to not use
deltas for anything except generating the timestream.


2. Move editor GUI to a separate module

Create an interface that just exposes the relevant streams and signals;
should allow a fairly elegant decoupling of GUI from the main loop. Probably
depends on #1, or at least having mouse events in carboxyl.

Note that there are some dependency issues that probably should be fixed.
Notably, world rendering and the GUI are ostensibly independent systems, but
have an implicit dependency on each other in that they both depend on the
scale signal and must use it in the same way in order for their integration to
be correct. This suggests that the dependency should be made explicit,
probably by moving ownership of the scale behavior to one or the other.

3. Move audio to a separate module, add basic mixing interface

3a. FRP audio mixer

Figure out a nice way to use FRP in the mixer. It would be super cool to
represent every channel as a reactive behavior, but this raises at least two
questions:

    1.  In a push-pull system, what is the "push" that induces the behavior
        to change?

    2.  When sampling from a channel, how does the underlying reactive
        behavior know how many samples to provide?

Both points are related to the necessarily discrete and chunked nature of
audio sampling. Currently an SDL audio callback is invoked at regular
intervals and expected to fill a buffer of a size known at the point where the
audio system is initialized.

Possibly, then, we could "push" a value into the audio network that indicates
how many samples will be required; each behavior (channel) could then produce
an appropraite number of samples from their sources.

NOTE: behaviors would also seem a natural way to implement transformations in
the audio mixer as well; e.g., an upsampling fold or low-pass filter, of
particular relevance to SPC emulator generated samples, as those are currently
fixed at 32KHz.

So, let's say we create a sink for "fill" events. An event would be pushed
into this sink by the audio callback.

What then depends on the stream, and how do the results get back to the audio
callback? How many channels are there in the mixer, and why? A limited set of
channels is a particularly poor abstraction. I wish to dispense with it. What
all can produce sound? How is a thing that can produce sound introduced to the
audio mixer?

Oh yeah, and one more neat idea: why not tie sound sampling to the rest of the
world's update cycle? 1/60s ~= 17ms is a plenty big enough audio buffer
length, and it seems to tie things together nicely.

There are some possible problems with this approach.

    1.  Timing. Starting the SDL audio system with a sample count
        corresponding to 1/60 of a second and relying on the audio callback
        is likely to misalign with the desired framerate.
    2.  Cumulative error. The SDL audio system provides an alternative, which
        is to push arbitrary quantities of samples to an internal buffer
        outside the control of the audio callback. This would allow us to push
        data at the beginning of every frame, but since the hardware is
        consuming it at its leisure, we are still likely to run into timing
        problems. If we push too few samples, the audio will have brief blank
        spaces, creating unpleasant ~60Hz noise. If we push too many, we fall
        increasingly out of sync.

It may be worth investigating lower-level access to audio hardware to address
some of these issues.

4. Greenfield a new FRP physics approach. Probably put momentum first.

Our first-order abstraction for physics entities will be a pair of signals,
momentum and position. (We can assume that in this system, σₓσₛ≠ħ/2) In the
normal course of things, position should be straightforwardly derived from
momentum. We'll implement just two momentum functions at first: constant,
and keyboard-controlled.

This should (hopefully) make it a bit easier to implement collision detection
in terms of momentum and position signals.
